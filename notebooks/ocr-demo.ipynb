{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6eaafa4",
   "metadata": {},
   "source": [
    "### Load libraries\n",
    "\n",
    "We'll use:\n",
    "- `pdf2image` to convert the pdf file to jpg images at 500dpi resolution\n",
    "- `argparse` to parse command-line arguments\n",
    "- `cv2` (OpenCV) to do image processing routines\n",
    "- `pytesseract` for out-of-the-box OCR (LSTM-based, popular with good docs)\n",
    "- `numpy` for standard arry manipulation\n",
    "- `json` for exporting the final dictionary\n",
    "- `re` for any parsing of text needed with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01934b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import argparse\n",
    "import cv2\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e062cffd",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c292928",
   "metadata": {},
   "source": [
    "#### Extract contours with a standard image-processing routine\n",
    "\n",
    "This procedure is like fancy low-pass filtering to bring large items into relief in a way that is friendly to document parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb6199dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contours(gray_img, gaussian_blur_dims: tuple, kernel_dims: tuple):\n",
    "    blur = cv2.GaussianBlur(gray_img, gaussian_blur_dims, 0)\n",
    "    thresh = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)[1]\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, kernel_dims)\n",
    "    dilate = cv2.dilate(thresh, kernel, iterations = 1)\n",
    "    cntrs = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cntrs = cntrs[0] if len(cntrs) == 2 else cntrs[1]\n",
    "    cntrs = sorted(cntrs, key = lambda x: cv2.boundingRect(x)[0])\n",
    "    return(cntrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40551b2f",
   "metadata": {},
   "source": [
    "#### Picking out vertical lines to demarcate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "822b6d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_vertical_line_bounding_boxes(raw_contours):\n",
    "    line_contours = [cv2.boundingRect(c) for c in raw_contours if \\\n",
    "                     cv2.boundingRect(c)[2] < 30 and \\\n",
    "                     cv2.boundingRect(c)[3] > 200 and \\\n",
    "                     cv2.boundingRect(c)[0] > 250]\n",
    "    return(line_contours)\n",
    "\n",
    "def grab_vertical_line_bounding_box(roi):\n",
    "    raw_contours = get_contours(roi, (5,21), (11, 21))\n",
    "    line_boxes = [cv2.boundingRect(c) for c in raw_contours if \\\n",
    "                  cv2.boundingRect(c)[2] < 30 and \\\n",
    "                  cv2.boundingRect(c)[3] > 200 and \\\n",
    "                  cv2.boundingRect(c)[0] > 300]\n",
    "    line_boxes = sorted(line_boxes, key = lambda x: x[3], reverse = True)\n",
    "    return(line_boxes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3efcd02d",
   "metadata": {},
   "source": [
    "#### Helper for determining if center of narrow line contour demargating columns is in larger block initially detected in first pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f3a571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_in_contour(point, contour):\n",
    "    return (\n",
    "        point[0] >= contour[0] and \\\n",
    "        point[0] <= contour[0] + contour[2] and \\\n",
    "        point[1] >= contour[1] and \\\n",
    "        point[1] <= contour[1] + contour[3] \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547ab384",
   "metadata": {},
   "source": [
    "#### Helper for splitting existing wide roi if vertical line lies inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70ca67a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list_item_rois_into_cols(list_item_rois):\n",
    "    split_list_rois = []\n",
    "    for roi in list_item_rois:\n",
    "        line_coords = grab_vertical_line_bounding_box(roi)\n",
    "        split_list_rois.append({\n",
    "            \"left\": roi[:, 0:line_coords[0]],\n",
    "            \"right\": roi[:, line_coords[0]:]\n",
    "        })\n",
    "    return(split_list_rois)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b96ff3",
   "metadata": {},
   "source": [
    "#### For parsing names/professions using post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "183b7b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_name_list(tess_output: str):\n",
    "    # Split by extracted lines and remove empties\n",
    "    name_list = [nm for nm in tess_output.split(\"\\n\") if len(nm) > 0]\n",
    "    # Remove category headers\n",
    "    name_list = [nm for nm in name_list if not nm.isupper()]\n",
    "    # If no comma in string, append to previous\n",
    "    new_name_list = []\n",
    "    last_item_idx = -1\n",
    "    for nm in name_list:\n",
    "        if last_item_idx >= 0:\n",
    "            if \",\" in nm:\n",
    "                new_name_list.append(nm)\n",
    "                last_item_idx += 1\n",
    "            else:\n",
    "                new_name_list[last_item_idx] = new_name_list[last_item_idx] + nm\n",
    "        else:\n",
    "            new_name_list.append(nm)\n",
    "            last_item_idx += 1\n",
    "    # Separate by comma, convert to dict\n",
    "    new_name_list = [{\"name\": nm[0], \"profession\": nm[1]} for nm in [n.split(\", \") for n in new_name_list]]\n",
    "    return(new_name_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241bd6c",
   "metadata": {},
   "source": [
    "### Demonstration using sample page, loading a small number of select pages from the original pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1414b14c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert your own path\n",
    "sample_path = 'book_excerpt.pdf'\n",
    "images = convert_from_path(sample_path, 500)\n",
    "\n",
    "img = images[0]\n",
    "\n",
    "img = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302c4db9",
   "metadata": {},
   "source": [
    "#### Detect how the page is divided horizontally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ea081ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "raw_wide_cntrs = get_contours(gray, (101, 31), (101, 31))\n",
    "wide_contours = [cv2.boundingRect(c) for c in raw_wide_cntrs if cv2.boundingRect(c)[2] > img.shape[1]/2]\n",
    "wide_rois = [img[y:y+h, x:x+w] for x,y,w,h in wide_contours]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee223ab",
   "metadata": {},
   "source": [
    "#### Within those horizontal divisions, which ones have vertical lines in them and which do not?\n",
    "\n",
    "Detect by looking in the center of the page for vertical lines (given size constraints, then see if the center of that line is in each of the rois."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c465d0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect line breaks demarcating columns\n",
    "raw_line_cntrs = get_contours(gray, (5,21), (11, 21))\n",
    "line_contours = grab_vertical_line_bounding_boxes(raw_line_cntrs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1862f3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate line contour centers, then determine which of wide blocks contains\n",
    "# list items that need parsing\n",
    "line_contour_centers = [(c[0] + c[2]/2, c[1] + c[3]/2) for c in line_contours]\n",
    "\n",
    "list_item_contours = [wc for wc in wide_contours if any([point_in_contour(lcc, wc) for lcc in line_contour_centers])]\n",
    "paragraph_contours = [wc for wc in wide_contours if not any([point_in_contour(lcc, wc) for lcc in line_contour_centers])]\n",
    "\n",
    "list_item_rois = [gray[y:y+h, x:x+w] for x,y,w,h in list_item_contours]\n",
    "paragraph_rois = [gray[y:y+h, x:x+w] for x,y,w,h in paragraph_contours]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9ec6b8",
   "metadata": {},
   "source": [
    "#### Do OCR on detected paragraph ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9e86f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from paragraphs\n",
    "paragraph_texts = [pytesseract.image_to_string(r, lang=\"enm\") for r in paragraph_rois]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b3ff3",
   "metadata": {},
   "source": [
    "#### Do further splitting, then OCR and postprocessing on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2946b104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each list-item roi, split into two, parse with pytesseract, reconcatenate\n",
    "split_list_item_rois = split_list_item_rois_into_cols(list_item_rois)\n",
    "\n",
    "list_item_texts = []\n",
    "for split_roi in split_list_item_rois:\n",
    "    list_item_texts.append(\n",
    "        pytesseract.image_to_string(split_roi[\"left\"], lang = \"enm\") + \"\\n\" + \n",
    "        pytesseract.image_to_string(split_roi[\"right\"], lang = \"enm\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc45580c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse list items with post-processing\n",
    "inhabitants = [parse_name_list(nm_ls) for nm_ls in list_item_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0029621a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring data together\n",
    "parsed_data = {\n",
    "    \"page\": 4, # first page of excerpt used is 4\n",
    "    \"text_block\": paragraph_texts,\n",
    "    \"data\": inhabitants\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af7ee472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save json\n",
    "with open('data.json', 'w') as fp:\n",
    "    json.dump(parsed_data, fp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
